{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPELL CHECKER APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a project to show how to implement spelling checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works? Some probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call correction(w) tries to choose the most likely spelling correction for w. There is no way to know for sure (for example, should \"lates\" be corrected to \"late\" or \"latest\" or \"lattes\" or ...?), which suggests we use probabilities. We are trying to find the correction c, out of all possible candidate corrections, that maximizes the probability that c is the intended correction, given the original word w:\n",
    "\n",
    "    argmaxc ∈ candidates P(c|w) \n",
    "\n",
    "By Bayes' Theorem this is equivalent to:\n",
    "\n",
    "    argmaxc ∈ candidates P(c) P(w|c) / P(w) \n",
    "\n",
    "Since P(w) is the same for every possible candidate c, we can factor it out, giving:\n",
    "\n",
    "    argmaxc ∈ candidates P(c) P(w|c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four parts of this expression are:\n",
    "\n",
    "    Selection Mechanism: argmax\n",
    "    We choose the candidate with the highest combined probability.\n",
    "\n",
    "    Candidate Model: c ∈ candidates\n",
    "    This tells us which candidate corrections, c, to consider.\n",
    "\n",
    "    Language Model: P(c)\n",
    "    The probability that c appears as a word of English text. For example, occurrences of \"the\" make up about 7% of English text, so we should have P(the) = 0.07.\n",
    "\n",
    "    Error Model: P(w|c)\n",
    "    The probability that w would be typed in a text when the author meant c. For example, P(teh|the) is relatively high, but P(theeexyz|the) would be very low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start with some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagisha/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: DeprecationWarning: the sets module is deprecated\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sets import Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating error matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the error model, we need to generate error matrix. \n",
    "We'll have insert matrix, delete matrix etc.\n",
    "\n",
    "For example : insertMatrix['a']['e'] means how many times from correct word to incorrect word, we have inserted an 'e' after an 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating the matrix, we need a list of common errors. Here I have used Peter Norwig's list of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_new = open('errors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ERRORS = file_new.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7843"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ERRORS.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ERRORS = ERRORS.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = ERRORS.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows list of first 5 in Peter Norwig's list of errors. \n",
    "raining is erroneously written as rainning and raning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raining: rainning, raning',\n",
       " 'writings: writtings',\n",
       " 'disparagingly: disparingly',\n",
       " 'yellow: yello',\n",
       " 'four: forer, fours, fuore, fore*5, for*4']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left = []\n",
    "right = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I split the errors into the correct word and the list of incorrect words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in x :\n",
    "    spl = i.split(':',1)\n",
    "    if len(spl)==2 :\n",
    "        left.append(spl[0])\n",
    "        right.append(spl[1])\n",
    "    count = count +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raining',\n",
       " 'writings',\n",
       " 'disparagingly',\n",
       " 'yellow',\n",
       " 'four',\n",
       " 'woods',\n",
       " 'hanging',\n",
       " 'aggression',\n",
       " 'looking',\n",
       " 'eligible']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' rainning, raning',\n",
       " ' writtings',\n",
       " ' disparingly',\n",
       " ' yello',\n",
       " ' forer, fours, fuore, fore*5, for*4',\n",
       " ' woodes',\n",
       " ' haing',\n",
       " ' agression',\n",
       " ' loking, begining, luing, look*2, locking, lucking, louk, looing, lookin, liking',\n",
       " ' eligble, elegable, eligable']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the error matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delMatrix = [[0]*26 for i in range(26)]\n",
    "insMatrix = [[0]*26 for i in range(26)]\n",
    "subMatrix = [[0]*26 for i in range(26)]\n",
    "excMatrix = [[0]*26 for i in range(26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = [\"a\", \"b\", \"c\", \"d\", \"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_ins(x,y) :\n",
    "    index_x = chars.index(x)\n",
    "    index_y = chars.index(y)\n",
    "    insMatrix[index_x][index_y] = insMatrix[index_x][index_y]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_del(x,y) :\n",
    "    index_x = chars.index(x)\n",
    "    index_y = chars.index(y)\n",
    "    #print(x,y)\n",
    "    delMatrix[index_x][index_y] = delMatrix[index_x][index_y]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_sub(x,y) :\n",
    "    index_x = chars.index(x)\n",
    "    index_y = chars.index(y)\n",
    "    subMatrix[index_x][index_y] = subMatrix[index_x][index_y]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_exc(x,y) :\n",
    "    index_x = chars.index(x)\n",
    "    index_y =chars.index(y)\n",
    "    excMatrix[index_x][index_y] = excMatrix[index_x][index_y]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The populateConfusionMatrix(word,errword) is the module to fill the error matrices given a word and it's errored word. It generates the edit distance matrix and the backtracks along the matrix to find what type of error has occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def populateConfusionMatrix(word,errword):\n",
    "    dp = [[0]*(len(errword)+1) for i in range(len(word)+1)]\n",
    "    m = len(word)+1;\n",
    "    n = len(errword)+1;\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            dp[i][0] = i;\n",
    "            dp[0][j] = j;\n",
    "    for i in range(m):\n",
    "        for j in range(n): \n",
    "            #print(i,j)\n",
    "            if i==0 or j==0 :\n",
    "                continue\n",
    "                \n",
    "            dis = [0]*4\n",
    "            dis[0] = dp[i-1][j]+1\n",
    "            dis[1] = dp[i][j-1]+1\n",
    "            #print(\"dis[1] : \" ,dp[i][j-1]+1)\n",
    "            if word[i-1] == errword[j-1]:\n",
    "                dis[2] = dp[i-1][j-1]\n",
    "            else :\n",
    "                dis[2] = dp[i-1][j-1]+1\n",
    "                \n",
    "            if i>1 and j>1 and word[i-1] == errword[j-2] and word[i-2] == errword[j-1]:\n",
    "                dis[3] = dp[i-2][j-2] + 1 \n",
    "            if dis[3]!=0 :\n",
    "                dp[i][j] = min(dis[0:4])\n",
    "            else :\n",
    "                dp[i][j] = min(dis[0:3])\n",
    "               \n",
    "    i = m-1\n",
    "    j = n-1\n",
    "    while(i>0 and j>0) :\n",
    "        #print(\"in while loop\")\n",
    "        if word[i-1] == errword[j-1] :\n",
    "            i=i-1\n",
    "            j=j-1\n",
    "            continue\n",
    "        if dp[i][j] == dp[i][j-1]+1 :\n",
    "            populate_ins(word[i-1],errword[j-1])\n",
    "            j=j-1\n",
    "        if dp[i][j] == dp[i-1][j]+1 :\n",
    "            populate_del(errword[j-1],word[i-1])\n",
    "            i=i-1\n",
    "        if dp[i][j] == dp[i-1][j-1] + 1 :\n",
    "            populate_sub(word[i-1],errword[j-1])\n",
    "            i=i-1\n",
    "            j=j-1\n",
    "        if i>1 and j>1 and word[i-1] == errword[j-2] and word[i-2] == errword[j-1] and dp[i][j] == dp[i-2][j-2]+1 :\n",
    "            populate_exc(word[i-2],word[i-1])\n",
    "            i=i-2\n",
    "            j=j-2\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the main module to fill matrices based on all errors in the given list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def errorPopulate():\n",
    "    symbol = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    star = \"*\"\n",
    "    size = len(left)\n",
    "    for i in range(size):\n",
    "        flag = 0\n",
    "        word = left[i]\n",
    "        for x in word :\n",
    "            if x not in symbol :\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag==1 :\n",
    "            continue\n",
    "        numErrorWords = len(right[i].split(\",\"))\n",
    "        allErrWords = right[i].split(\",\")\n",
    "        for j in range(numErrorWords) :\n",
    "            errorWord = allErrWords[j]\n",
    "            for k in range(len(errorWord)):\n",
    "                if errorWord[k] == \" \":\n",
    "                    continue\n",
    "                else :\n",
    "                    break        \n",
    "            errorWord = errorWord[k:]   \n",
    "            flag2 =0\n",
    "            for x in errorWord :\n",
    "                if x not in symbol :\n",
    "                    flag2 = 1\n",
    "                    break\n",
    "            if flag2==1 :\n",
    "                continue\n",
    "            print(\"\\n\"+errorWord)\n",
    "            size = 1\n",
    "            for k in range(len(errorWord)) :\n",
    "                if errorWord[k] == \"*\" :\n",
    "                    size = int(errorWord[k+1:])\n",
    "                    errorWord = errorWord[0:k]\n",
    "                    break\n",
    "                else :\n",
    "                    continue\n",
    "            while size>0 :\n",
    "                print(\"Populating \"+ word+ \"and\"+ errorWord)\n",
    "                populateConfusionMatrix(word,errorWord)\n",
    "                print(\"Complete\")\n",
    "                size = size-1\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these matrices are to be saved because these computations need not be done everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"insertMatrix\",insMatrix)\n",
    "np.savetxt(\"deleteMatrix\",delMatrix)\n",
    "np.savetxt(\"substituteMatrix\",subMatrix)\n",
    "np.savetxt(\"exchangeMatrix\",excMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data : Text and Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some words to generate the language model.\n",
    "The list WORDS is a list of the words in the TEXT, but it can also serve as a generative model of text. We know that language is very complicated, but we can create a simplified model of language that captures part of the complexity. In the bag of words model, we ignore the order of words, but maintain their frequency.\n",
    "\n",
    "Language Model: P(c)\n",
    "The probability that c appears as a word of English text. For example, occurrences of \"the\" make up about 7% of English text, so we should have P(the) = 0.07."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need some text, possibly from a file. Then we can break the text into words. Here I am using Peter Norwig's big.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_new = open('words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = file_new.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following module breaks the given text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall('[a-z]+',text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words do we have? It's more than a million words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1105285"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize(TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORDS = tokenize(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes']\n"
     ]
    }
   ],
   "source": [
    "print(WORDS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One representation for a bag of words is a Counter, which is a dictionary of {'word': count} pairs. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 2, 'is': 2, 'it': 1, 'test': 2, 'this': 1})"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokenize('Is this a test? It is a test!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = Counter(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 80030),\n",
       " ('of', 40025),\n",
       " ('and', 38313),\n",
       " ('to', 28766),\n",
       " ('in', 22050),\n",
       " ('a', 21155),\n",
       " ('that', 12512),\n",
       " ('he', 12401),\n",
       " ('was', 11410),\n",
       " ('it', 10681),\n",
       " ('his', 10034),\n",
       " ('is', 9774),\n",
       " ('with', 9740),\n",
       " ('as', 8064),\n",
       " ('i', 7687)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generating Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we are only considering words which are 0 or 1 edit distance away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splits(word) :\n",
    "    return [(word[:i],word[i:]) for i in range(len(word)+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The splits(word) function splits up the word into two at every location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'wird'), ('w', 'ird'), ('wi', 'rd'), ('wir', 'd'), ('wird', '')]\n"
     ]
    }
   ],
   "source": [
    "print(splits(\"wird\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates all 1-edit distance away words frome the given words, the 1-edit distance being achievable from either insert, delete, replace or transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits1(word) :\n",
    "    pairs = splits(word)\n",
    "    inserts= [a+c+b for (a,b) in pairs for c in chars]\n",
    "    deletes= [a+b[1:] for (a,b) in pairs if len(b)>0]\n",
    "    replaces = [a+c+b[1:] for (a,b) in pairs for c in chars if len(b)>0]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a,b) in pairs if len(b)>1]\n",
    "    \n",
    "    return set(inserts+deletes+replaces+transposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n"
     ]
    }
   ],
   "source": [
    "print(len(edits1(\"wird\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edits0(word):\n",
    "    return set([word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every 1-edit distance away word is not useful. We need only those words which make sense, or are in the given dictionary. Therefore, here we generate a subset of these words by checking if they are in our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inDictionary(words) :\n",
    "    return {w for w in words if w in count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['wire', 'word', 'sird', 'wirt', 'gird', 'wid', 'wired', 'wild', 'weird', 'ward', 'bird', 'wind', 'wiry'])\n"
     ]
    }
   ],
   "source": [
    "print(inDictionary(edits1(\"wird\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language model we are using is basically just the probability of the word in the given text.\n",
    "So, the module given below returns this probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def probLangModel(word) :\n",
    "    N = sum(count.values())\n",
    "    return float(float(count[word])/N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.16660408853825e-05"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probLangModel(\"wild\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0724066643445 the\n",
      "0.00884296810325 is\n",
      "0.000821507574969 most\n",
      "0.00025966153526 common\n",
      "0.000269613719538 word\n",
      "0.0199496057578 in\n",
      "0.000190900989338 english\n"
     ]
    }
   ],
   "source": [
    "for w in tokenize('\"The\" is most common word in English'):\n",
    "    print probLangModel(w), w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Error Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the error matrices generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "insArray = np.loadtxt(\"insertMatrix\")\n",
    "delArray = np.loadtxt(\"deleteMatrix\")\n",
    "subArray = np.loadtxt(\"substituteMatrix\")\n",
    "excArray = np.loadtxt(\"exchangeMatrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to find what type of error has occurred and where. For example if \"ready\" is written as \"read\", this module will tell us that a deletion has occurred and that a 'y' has been deleted after a 'd'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def errorType(word,errword):\n",
    "    if word == errword :\n",
    "        return \"no-op\"\n",
    "    dp = [[0]*(len(errword)+1) for i in range(len(word)+1)]\n",
    "    m = len(word)+1;\n",
    "    n = len(errword)+1;\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            dp[i][0] = i;\n",
    "            dp[0][j] = j;\n",
    "    for i in range(m):\n",
    "        for j in range(n): \n",
    "            #print(i,j)\n",
    "            if i==0 or j==0 :\n",
    "                continue\n",
    "                \n",
    "            dis = [0]*4\n",
    "            dis[0] = dp[i-1][j]+1\n",
    "            dis[1] = dp[i][j-1]+1\n",
    "            #print(\"dis[1] : \" ,dp[i][j-1]+1)\n",
    "            if word[i-1] == errword[j-1]:\n",
    "                dis[2] = dp[i-1][j-1]\n",
    "            else :\n",
    "                dis[2] = dp[i-1][j-1]+1\n",
    "                \n",
    "            if i>1 and j>1 and word[i-1] == errword[j-2] and word[i-2] == errword[j-1]:\n",
    "                dis[3] = dp[i-2][j-2] + 1 \n",
    "            if dis[3]!=0 :\n",
    "                dp[i][j] = min(dis[0:4])\n",
    "            else :\n",
    "                dp[i][j] = min(dis[0:3])\n",
    "                \n",
    "    if dp[m-1][n-1] == 1 :\n",
    "        #return 1 operation\n",
    "        i = m-1\n",
    "        j = n-1\n",
    "        while(i>0 and j>0) :\n",
    "        #print(\"in while loop\")\n",
    "            if word[i-1] == errword[j-1] :\n",
    "                i=i-1\n",
    "                j=j-1\n",
    "                continue\n",
    "            if dp[i][j] == dp[i][j-1]+1 :\n",
    "                return \"insert:\"+word[i-1]+\":\"+errword[j-1]\n",
    "                #continue insert\n",
    "            if dp[i][j] == dp[i-1][j]+1 :\n",
    "            #print(i,j) delete\n",
    "                return \"delete:\"+errword[j-1]+\":\"+word[i-1]\n",
    "            #continue\n",
    "            if dp[i][j] == dp[i-1][j-1] + 1 :\n",
    "                return \"sub:\"+word[i-1]+\":\"+errword[j-1]\n",
    "            #continue\n",
    "            if i>1 and j>1 and word[i-1] == errword[j-2] and word[i-2] == errword[j-1] and dp[i][j] == dp[i-2][j-2]+1 :\n",
    "                return \"exchange:\"+word[i-1]+\":\"+word[i-2]\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example for the above function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delete:d:y'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorType(\"ready\",\"read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function first determines the error type and then finds the value of the error matrix on the type and error occured. For example : \"ready\" to \"read\" will find the count of delArray['d']['y']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def errorCount(x,y) :\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    operation = errorType(x,y)\n",
    "    if operation == \"no-op\" :\n",
    "        return 1.0\n",
    "    else :\n",
    "        text = operation.split(\":\")\n",
    "        if text[0] == \"insert\" :\n",
    "            #insert probability\n",
    "            index1 = alphabet.index(text[1])\n",
    "            index2 = alphabet.index(text[2])\n",
    "            return insArray[index1][index2]\n",
    "        elif text[0] == \"delete\" :\n",
    "            #delete probability\n",
    "            index1 = alphabet.index(text[1])\n",
    "            index2 = alphabet.index(text[2])\n",
    "            return delArray[index1][index2]\n",
    "        elif text[0] == \"sub\" :\n",
    "            #substitute probability\n",
    "            index1 = alphabet.index(text[1])\n",
    "            index2 = alphabet.index(text[2])\n",
    "            return subArray[index1][index2]\n",
    "        elif text[0] == \"exchange\" :\n",
    "            #exchange probability\n",
    "            index1 = alphabet.index(text[1])\n",
    "            index2 = alphabet.index(text[2])\n",
    "            return excArray[index1][index2]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally we need to find the probability of the error occurred. For this, we will divide the count generated from the above function by the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def probErrorModel(x,y):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    operation = errorType(x,y)\n",
    "    #count = errorCount(x,y)\n",
    "    #print(count)\n",
    "    \n",
    "    if operation == \"no-op\" :\n",
    "        return 1.0\n",
    "    elif operation is None :\n",
    "        return 0.000001\n",
    "    else:\n",
    "\n",
    "        text = operation.split(\":\")\n",
    "        count = errorCount(x,y)\n",
    "        index = alphabet.index(text[1])\n",
    "        sum = 0\n",
    "        for i in range(26):\n",
    "            sum += insArray[index][i]\n",
    "            sum += delArray[index][i]\n",
    "            sum += subArray[index][i]\n",
    "            sum += excArray[index][i]\n",
    "         \n",
    "        #print(sum)\n",
    "        return count/sum   \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of error, writing \"read\" instead of \"ready\" is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010299166257969592"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probErrorModel(\"ready\",\"read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Error Model and Language Model and generating a list of top candidates for the given erroneous word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since P(w) is the same for every possible candidate c, we can factor it out, giving:\n",
    "\n",
    "argmaxc ∈ candidates P(c) P(w|c) \n",
    "\n",
    "Thus, multiplying both the probabilities, and sorting in decreasing order, we can find the top candidate given by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getkey(keys) :\n",
    "    return keys[1]\n",
    "\n",
    "def gencorrectWords(word) :\n",
    "    relatedWords = inDictionary(edits0(word))|inDictionary(edits1(word))\n",
    "    #print(relatedWords)\n",
    "    listOfWords = []\n",
    "    for w in relatedWords:\n",
    "        #print(word,w)\n",
    "        probability = probErrorModel(w,word)*probLangModel(w)\n",
    "        listOfWords.append((w,probability))\n",
    "    #print(listOfWords)\n",
    "    listOfWords = sorted(listOfWords,key=getkey,reverse=True)\n",
    "    return listOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this out, by giving the erroneous word \"maet\" , we find the top candidate to be \"meet\" by this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meet', 1.8954854861327568e-05),\n",
       " ('met', 7.8550476652382218e-06),\n",
       " ('mast', 6.2892591373217227e-08),\n",
       " ('malt', 5.7743956908638683e-08),\n",
       " ('mat', 4.0691030914396161e-08),\n",
       " ('matt', 3.5996263311849448e-08),\n",
       " ('mate', 9.208951087422766e-09),\n",
       " ('meat', 8.4188339822888612e-09),\n",
       " ('aet', 3.6189761011865716e-11)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gencorrectWords(\"maet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest implementation of a spell - checker.\n",
    "\n",
    "There are various issues to be looked into.\n",
    "\n",
    "1. Instead of using Unigram model, we can use Bigram or Trigram model which could effectively take into account the surrounding words of the given erroneous word.\n",
    "\n",
    "2. Here if our dictionary does not contain a word, then it's probability of occurence is taken as zero, which is unnecessarily stringent. Our dictionary might miss certain words.We can address this by assigning a non-zero probability to words that are not in the dictionary. This is even more important when it comes to multi-word phrases (such as bigrams), because it is more likely that a legitimate one will appear that has not been observed before.We can think of our model as being overly spiky; it has a spike of probability mass wherever a word or phrase occurs in the corpus. What we would like to do is smooth over those spikes so that we get a model that does not depend on the details of our corpus. The process of \"fixing\" the model is called smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These two issues will be addressed in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
